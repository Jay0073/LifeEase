{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b57d50ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "import torch\n",
    "import gc\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4748b026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668f7b3d862c45a2a4bd62f41feb072f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba2fa72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup YOLO for Object Detection ---\n",
    "def setup_yolo(model_path: str) -> ort.InferenceSession:\n",
    "    try:\n",
    "        # Initialize ONNX runtime session\n",
    "        session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])  # Use 'NNAPIExecutionProvider' for Android\n",
    "        return session\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up YOLO: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89300d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup Gemma with llama_cpp ---\n",
    "def setup_gemma():\n",
    "    os.environ['LLAMA_NUMA'] = '1'\n",
    "    os.environ['LLAMA_MMX_NTHREADS'] = '8'\n",
    "\n",
    "    model = Llama(\n",
    "        model_path=r\"C:\\Users\\voutl\\OneDrive\\Documents\\LifeEase\\gemma-2-2b-it-Q5_K_M.gguf\",  # Ensure this path is correct\n",
    "        n_ctx=512,\n",
    "        n_threads=8,\n",
    "        n_gpu_layers=-1 \n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c9a0e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup BLIP for Image Captioning ---\n",
    "def setup_blip():\n",
    "    try:\n",
    "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        # Move to GPU if available, else CPU (mobile may need CPU-only)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        return processor, model, device\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up BLIP: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85e45ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocess Image for YOLO ---\n",
    "def preprocess_yolo(image_path: str, input_size: Tuple[int, int] = (640, 640)) -> np.ndarray:\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        ratio = min(input_size[0] / w, input_size[1] / h)\n",
    "        new_w, new_h = int(w * ratio), int(h * ratio)\n",
    "        image = cv2.resize(image, (new_w, new_h))\n",
    "        \n",
    "        padded = np.zeros((input_size[1], input_size[0], 3), dtype=np.uint8)\n",
    "        padded[:new_h, :new_w] = image\n",
    "        image = padded\n",
    "        \n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        return image, (w, h), ratio\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing image for YOLO: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a813adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Postprocess YOLO Outputs ---\n",
    "def postprocess_yolo(outputs: np.ndarray, original_size: Tuple[int, int], ratio: float, conf_thres: float = 0.4, iou_thres: float = 0.5) -> List[dict]:\n",
    "    try:\n",
    "        detections = outputs[0]  # Shape: (1, 84, 8400)\n",
    "        print(f\"Raw detections shape: {detections.shape}\")\n",
    "        \n",
    "        # Squeeze and transpose to (8400, 84)\n",
    "        detections = np.transpose(np.squeeze(detections), (1, 0))  # (8400, 84)\n",
    "        print(f\"Reshaped detections shape: {detections.shape}\")\n",
    "        \n",
    "        if detections.size == 0:\n",
    "            print(\"No detections found.\")\n",
    "            return []\n",
    "\n",
    "        boxes, scores, classes = [], [], []\n",
    "        for det in detections:\n",
    "            # Extract box coordinates, confidence, and class scores\n",
    "            x_center, y_center, width, height, conf = det[:5]\n",
    "            class_scores = det[5:]  # 80 class probabilities\n",
    "            class_id = np.argmax(class_scores)\n",
    "            score = conf * class_scores[class_id]  # Combined confidence\n",
    "            \n",
    "            if score > conf_thres:\n",
    "                # Convert to corner coordinates\n",
    "                x = (x_center - width / 2) / ratio\n",
    "                y = (y_center - height / 2) / ratio\n",
    "                w = width / ratio\n",
    "                h = height / ratio\n",
    "                boxes.append([x, y, x + w, y + h])  # [x_min, y_min, x_max, y_max]\n",
    "                scores.append(float(score))\n",
    "                classes.append(int(class_id))\n",
    "\n",
    "        if not boxes:\n",
    "            return []\n",
    "\n",
    "        # Simple NMS (approximation using NumPy)\n",
    "        indices = np.argsort(scores)[::-1]\n",
    "        suppress = set()\n",
    "        for i in range(len(indices)):\n",
    "            if i in suppress:\n",
    "                continue\n",
    "            for j in range(i + 1, len(indices)):\n",
    "                if j in suppress:\n",
    "                    continue\n",
    "                iou = compute_iou(boxes[indices[i]], boxes[indices[j]])\n",
    "                if iou > iou_thres:\n",
    "                    suppress.add(j)\n",
    "\n",
    "        # Filter valid detections\n",
    "        valid_indices = [i for i in indices if i not in suppress]\n",
    "        results = []\n",
    "        class_names = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
    "                       \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
    "                       \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
    "                       \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "                       \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "                       \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
    "                       \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "                       \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
    "                       \"hair drier\", \"toothbrush\"]  # COCO 80 classes\n",
    "        for i in valid_indices:\n",
    "            x, y, x_max, y_max = boxes[i]\n",
    "            results.append({\n",
    "                \"class\": class_names[classes[i]],\n",
    "                \"confidence\": scores[i],\n",
    "                \"box\": {\"x\": float(x), \"y\": float(y), \"w\": float(x_max - x), \"h\": float(y_max - y)}\n",
    "            })\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error postprocessing YOLO outputs: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10cf39fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function for IoU ---\n",
    "def compute_iou(box1: List[float], box2: List[float]) -> float:\n",
    "    x1, y1, x1_max, y1_max = box1\n",
    "    x2, y2, x2_max, y2_max = box2\n",
    "    \n",
    "    inter_x1 = max(x1, x2)\n",
    "    inter_y1 = max(y1, y2)\n",
    "    inter_x2 = min(x1_max, x2_max)\n",
    "    inter_y2 = min(y1_max, y2_max)\n",
    "    \n",
    "    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "    box1_area = (x1_max - x1) * (y1_max - y1)\n",
    "    box2_area = (x2_max - x2) * (y2_max - y2)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    return inter_area / union_area if union_area > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8239567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modified Generate Response with YOLO ---\n",
    "def generate_response(gemma_model, blip_processor=None, blip_model=None, yolo_session=None, device=None, user_input=None, image_path=None):\n",
    "    caption, yolo_results = None, []\n",
    "    \n",
    "    if image_path and blip_processor and blip_model:\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            inputs = blip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                caption_ids = blip_model.generate(**inputs, max_length=50)\n",
    "            caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "            print(f\"BLIP Caption: {caption}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image with BLIP: {e}\")\n",
    "            caption = \"Image processing failed.\"\n",
    "        \n",
    "        if yolo_session:\n",
    "            try:\n",
    "                image, original_size, ratio = preprocess_yolo(image_path)\n",
    "                if image is not None:\n",
    "                    inputs = {yolo_session.get_inputs()[0].name: image}\n",
    "                    outputs = yolo_session.run(None, inputs)\n",
    "                    yolo_results = postprocess_yolo(outputs, original_size, ratio)\n",
    "                    print(f\"YOLO Results: {yolo_results}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image with YOLO: {e}\")\n",
    "        \n",
    "        yolo_desc = \"\"\n",
    "        if yolo_results:\n",
    "            yolo_desc = \"Objects detected: \" + \"; \".join(\n",
    "                [f\"{res['class']} at ({res['box']['x']:.1f}, {res['box']['y']:.1f})\" for res in yolo_results]\n",
    "            )\n",
    "        prompt = f\"<start_of_turn>user\\nDescribe the scene based only on the caption and detected objects with their approximate locations. Provide a short, factual description without poetic language or unnecessary details: {caption}. Objects detected: {yolo_desc}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    elif user_input:\n",
    "        prompt = f\"<start_of_turn>user\\n{user_input}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    else:\n",
    "        return \"Please provide text or an image.\"\n",
    "\n",
    "    response = \"\"\n",
    "    try:\n",
    "        for chunk in gemma_model(\n",
    "            prompt,\n",
    "            max_tokens=500,\n",
    "            temperature=0.7,\n",
    "            stream=True\n",
    "        ):\n",
    "            text_chunk = chunk[\"choices\"][0][\"text\"]\n",
    "            response += text_chunk\n",
    "            print(text_chunk, end=\"\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e1142491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cleanup Resources ---\n",
    "def cleanup(gemma_model, blip_model=None):\n",
    "    # Cleanup Gemma\n",
    "    gemma_model.reset()\n",
    "    del gemma_model\n",
    "    \n",
    "    # Cleanup BLIP if loaded\n",
    "    if blip_model:\n",
    "        del blip_model\n",
    "        torch.cuda.empty_cache()  # Clear GPU memory if used\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93bdf86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modified Main Function ---\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize models\n",
    "        gemma_model = setup_gemma()\n",
    "        blip_processor, blip_model, device = setup_blip()\n",
    "        yolo_session = setup_yolo(r\"C:\\Users\\voutl\\OneDrive\\Documents\\LifeEase\\yolov8n.onnx\")  # Replace with your YOLO ONNX model path\n",
    "\n",
    "        # Example: Image input\n",
    "        print(\"\\n--- Image Example ---\")\n",
    "        image_path = r\"C:\\Users\\voutl\\OneDrive\\Desktop\\download (1).jpg\"\n",
    "        response = generate_response(\n",
    "            gemma_model,\n",
    "            blip_processor,\n",
    "            blip_model,\n",
    "            yolo_session,\n",
    "            device,\n",
    "            image_path=image_path\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        cleanup(gemma_model, blip_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5f983",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c16bb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 39 key-value pairs and 288 tensors from C:\\Users\\voutl\\OneDrive\\Documents\\LifeEase\\gemma-2-2b-it-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = it\n",
      "llama_model_loader: - kv   4:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gemma\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"conversational\", \"text-generation\"]\n",
      "llama_model_loader: - kv   8:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv  10:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  11:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  12:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  13:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  14:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  16:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  18:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  19:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  20:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/gemma-2-2b-it-GGUF/gemma-...\n",
      "llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 182\n",
      "llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type q5_K:  156 tensors\n",
      "llama_model_loader: - type q6_K:   27 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Medium\n",
      "print_info: file size   = 1.79 GiB (5.87 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      1 '<eos>' is not marked as EOG\n",
      "load: control token:      0 '<pad>' is not marked as EOG\n",
      "load: control token:     47 '<unused40>' is not marked as EOG\n",
      "load: control token:      3 '<unk>' is not marked as EOG\n",
      "load: control token:     55 '<unused48>' is not marked as EOG\n",
      "load: control token:     24 '<unused17>' is not marked as EOG\n",
      "load: control token:      2 '<bos>' is not marked as EOG\n",
      "load: control token:      5 '<2mass>' is not marked as EOG\n",
      "load: control token:     58 '<unused51>' is not marked as EOG\n",
      "load: control token:      4 '<mask>' is not marked as EOG\n",
      "load: control token:     42 '<unused35>' is not marked as EOG\n",
      "load: control token:      9 '<unused2>' is not marked as EOG\n",
      "load: control token:      6 '[@BOS@]' is not marked as EOG\n",
      "load: control token:     44 '<unused37>' is not marked as EOG\n",
      "load: control token:     35 '<unused28>' is not marked as EOG\n",
      "load: control token:      7 '<unused0>' is not marked as EOG\n",
      "load: control token:     43 '<unused36>' is not marked as EOG\n",
      "load: control token:     36 '<unused29>' is not marked as EOG\n",
      "load: control token:      8 '<unused1>' is not marked as EOG\n",
      "load: control token:     41 '<unused34>' is not marked as EOG\n",
      "load: control token:     10 '<unused3>' is not marked as EOG\n",
      "load: control token:     40 '<unused33>' is not marked as EOG\n",
      "load: control token:     11 '<unused4>' is not marked as EOG\n",
      "load: control token:     39 '<unused32>' is not marked as EOG\n",
      "load: control token:     12 '<unused5>' is not marked as EOG\n",
      "load: control token:     38 '<unused31>' is not marked as EOG\n",
      "load: control token:     13 '<unused6>' is not marked as EOG\n",
      "load: control token:     37 '<unused30>' is not marked as EOG\n",
      "load: control token:     14 '<unused7>' is not marked as EOG\n",
      "load: control token:     27 '<unused20>' is not marked as EOG\n",
      "load: control token:     15 '<unused8>' is not marked as EOG\n",
      "load: control token:     28 '<unused21>' is not marked as EOG\n",
      "load: control token:     16 '<unused9>' is not marked as EOG\n",
      "load: control token:     17 '<unused10>' is not marked as EOG\n",
      "load: control token:     18 '<unused11>' is not marked as EOG\n",
      "load: control token:     19 '<unused12>' is not marked as EOG\n",
      "load: control token:     20 '<unused13>' is not marked as EOG\n",
      "load: control token:     21 '<unused14>' is not marked as EOG\n",
      "load: control token:     22 '<unused15>' is not marked as EOG\n",
      "load: control token:     56 '<unused49>' is not marked as EOG\n",
      "load: control token:     23 '<unused16>' is not marked as EOG\n",
      "load: control token:     54 '<unused47>' is not marked as EOG\n",
      "load: control token:     25 '<unused18>' is not marked as EOG\n",
      "load: control token:     53 '<unused46>' is not marked as EOG\n",
      "load: control token:     26 '<unused19>' is not marked as EOG\n",
      "load: control token:     29 '<unused22>' is not marked as EOG\n",
      "load: control token:     30 '<unused23>' is not marked as EOG\n",
      "load: control token:     31 '<unused24>' is not marked as EOG\n",
      "load: control token:     32 '<unused25>' is not marked as EOG\n",
      "load: control token:     46 '<unused39>' is not marked as EOG\n",
      "load: control token:     33 '<unused26>' is not marked as EOG\n",
      "load: control token:     45 '<unused38>' is not marked as EOG\n",
      "load: control token:     34 '<unused27>' is not marked as EOG\n",
      "load: control token:     48 '<unused41>' is not marked as EOG\n",
      "load: control token:     49 '<unused42>' is not marked as EOG\n",
      "load: control token:     50 '<unused43>' is not marked as EOG\n",
      "load: control token:     51 '<unused44>' is not marked as EOG\n",
      "load: control token:     52 '<unused45>' is not marked as EOG\n",
      "load: control token:     57 '<unused50>' is not marked as EOG\n",
      "load: control token:     59 '<unused52>' is not marked as EOG\n",
      "load: control token:     60 '<unused53>' is not marked as EOG\n",
      "load: control token:     61 '<unused54>' is not marked as EOG\n",
      "load: control token:     62 '<unused55>' is not marked as EOG\n",
      "load: control token:     63 '<unused56>' is not marked as EOG\n",
      "load: control token:     64 '<unused57>' is not marked as EOG\n",
      "load: control token:     65 '<unused58>' is not marked as EOG\n",
      "load: control token:     66 '<unused59>' is not marked as EOG\n",
      "load: control token:     67 '<unused60>' is not marked as EOG\n",
      "load: control token:     68 '<unused61>' is not marked as EOG\n",
      "load: control token:     69 '<unused62>' is not marked as EOG\n",
      "load: control token:     70 '<unused63>' is not marked as EOG\n",
      "load: control token:     71 '<unused64>' is not marked as EOG\n",
      "load: control token:     72 '<unused65>' is not marked as EOG\n",
      "load: control token:     73 '<unused66>' is not marked as EOG\n",
      "load: control token:     74 '<unused67>' is not marked as EOG\n",
      "load: control token:     75 '<unused68>' is not marked as EOG\n",
      "load: control token:     76 '<unused69>' is not marked as EOG\n",
      "load: control token:     77 '<unused70>' is not marked as EOG\n",
      "load: control token:     78 '<unused71>' is not marked as EOG\n",
      "load: control token:     79 '<unused72>' is not marked as EOG\n",
      "load: control token:     80 '<unused73>' is not marked as EOG\n",
      "load: control token:     81 '<unused74>' is not marked as EOG\n",
      "load: control token:     82 '<unused75>' is not marked as EOG\n",
      "load: control token:     83 '<unused76>' is not marked as EOG\n",
      "load: control token:     84 '<unused77>' is not marked as EOG\n",
      "load: control token:     85 '<unused78>' is not marked as EOG\n",
      "load: control token:     86 '<unused79>' is not marked as EOG\n",
      "load: control token:     87 '<unused80>' is not marked as EOG\n",
      "load: control token:     88 '<unused81>' is not marked as EOG\n",
      "load: control token:     89 '<unused82>' is not marked as EOG\n",
      "load: control token:     90 '<unused83>' is not marked as EOG\n",
      "load: control token:     91 '<unused84>' is not marked as EOG\n",
      "load: control token:     92 '<unused85>' is not marked as EOG\n",
      "load: control token:     93 '<unused86>' is not marked as EOG\n",
      "load: control token:     94 '<unused87>' is not marked as EOG\n",
      "load: control token:     95 '<unused88>' is not marked as EOG\n",
      "load: control token:     96 '<unused89>' is not marked as EOG\n",
      "load: control token:     97 '<unused90>' is not marked as EOG\n",
      "load: control token:     98 '<unused91>' is not marked as EOG\n",
      "load: control token:     99 '<unused92>' is not marked as EOG\n",
      "load: control token:    100 '<unused93>' is not marked as EOG\n",
      "load: control token:    101 '<unused94>' is not marked as EOG\n",
      "load: control token:    102 '<unused95>' is not marked as EOG\n",
      "load: control token:    103 '<unused96>' is not marked as EOG\n",
      "load: control token:    104 '<unused97>' is not marked as EOG\n",
      "load: control token:    105 '<unused98>' is not marked as EOG\n",
      "load: control token:    106 '<start_of_turn>' is not marked as EOG\n",
      "load: control token: 255999 '<unused99>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 249\n",
      "load: token to piece cache size = 1.6014 MB\n",
      "print_info: arch             = gemma2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 2304\n",
      "print_info: n_layer          = 26\n",
      "print_info: n_head           = 8\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 256\n",
      "print_info: n_swa            = 4096\n",
      "print_info: n_embd_head_k    = 256\n",
      "print_info: n_embd_head_v    = 256\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 9216\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 2B\n",
      "print_info: model params     = 2.61 B\n",
      "print_info: general.name     = Gemma 2 2b It\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 256000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 2 '<bos>'\n",
      "print_info: EOS token        = 1 '<eos>'\n",
      "print_info: EOT token        = 107 '<end_of_turn>'\n",
      "print_info: UNK token        = 3 '<unk>'\n",
      "print_info: PAD token        = 0 '<pad>'\n",
      "print_info: LF token         = 227 '<0x0A>'\n",
      "print_info: EOG token        = 1 '<eos>'\n",
      "print_info: EOG token        = 107 '<end_of_turn>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 288 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  1828.42 MiB\n",
      "............................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 26, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    52.00 MiB\n",
      "llama_init_from_model: KV self size  =   52.00 MiB, K (f16):   26.00 MiB, V (f16):   26.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   504.50 MiB\n",
      "llama_init_from_model: graph nodes  = 1050\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Gemma 2 2b It', 'general.architecture': 'gemma2', 'general.type': 'model', 'general.basename': 'gemma-2', 'general.finetune': 'it', 'general.size_label': '2B', 'gemma2.context_length': '8192', 'general.license': 'gemma', 'gemma2.embedding_length': '2304', 'gemma2.block_count': '26', 'gemma2.feed_forward_length': '9216', 'gemma2.attention.head_count': '8', 'gemma2.attention.head_count_kv': '4', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'tokenizer.ggml.eos_token_id': '1', 'gemma2.attn_logit_softcapping': '50.000000', 'general.file_type': '17', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.sliding_window': '4096', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.file': '/models_out/gemma-2-2b-it-GGUF/gemma-2-2b-it.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.entries_count': '182'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Image Example ---\n",
      "BLIP Caption: a street scene with a man walking down the street\n",
      "Raw detections shape: (1, 84, 8400)\n",
      "Reshaped detections shape: (8400, 84)\n",
      "YOLO Results: []\n",
      "Please provide the caption! ðŸ˜Š  I need the text to describe the scene and identify the objects. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3968.55 ms\n",
      "llama_perf_context_print: prompt eval time =    3967.72 ms /    53 tokens (   74.86 ms per token,    13.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1870.62 ms /    22 runs   (   85.03 ms per token,    11.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    5936.33 ms /    75 tokens\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnxEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
