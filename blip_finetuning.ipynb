{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cef7a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (8.1.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipywidgets) (8.12.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipywidgets) (3.0.14)\n",
      "Requirement already satisfied: backcall in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\voutl\\.conda\\envs\\onnxenv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0926296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import requests # To handle potential image loading errors (less likely now)\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DefaultDataCollator\n",
    ")\n",
    "# from datasets import load_dataset # No longer using huggingface datasets loader\n",
    "import os\n",
    "import json\n",
    "import traceback # For error details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a801ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Base BLIP model for image captioning\n",
    "MODEL_ID = \"Salesforce/blip-image-captioning-base\"\n",
    "# --- Local Dataset Paths (IMPORTANT: Update these paths) ---\n",
    "# Assumes a structure like:\n",
    "# /path/to/your/dataset/\n",
    "#   L train/\n",
    "#   L val/\n",
    "#  L test/\n",
    "#  L annotations/\n",
    "#      L train.json\n",
    "#      L val.json\n",
    "#      L test.json\n",
    "DATASET_ROOT_DIR = r\"C:\\Users\\voutl\\OneDrive\\Documents\\LifeEase\\vizwiz dataset\" # CHANGE THIS\n",
    "ANNOTATIONS_DIR = os.path.join(DATASET_ROOT_DIR, \"annotations\")\n",
    "TRAIN_IMG_DIR = os.path.join(DATASET_ROOT_DIR, \"train\")\n",
    "VAL_IMG_DIR = os.path.join(DATASET_ROOT_DIR, \"val\")\n",
    "TRAIN_ANNOTATION_FILE = os.path.join(ANNOTATIONS_DIR, \"train.json\")\n",
    "VAL_ANNOTATION_FILE = os.path.join(ANNOTATIONS_DIR, \"val.json\")\n",
    "# -------------------------------------------------------------\n",
    "# Directory to save the fine-tuned model and training outputs\n",
    "OUTPUT_DIR = \"./blip_finetuned_vizwiz_local\"\n",
    "# Training hyperparameters (adjust as needed)\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 8 # Adjust based on GPU memory\n",
    "NUM_EPOCHS = 3 # Adjust number of training epochs\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOGGING_STEPS = 100 # Log training progress every N steps\n",
    "SAVE_STEPS = 500 # Save checkpoint every N steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5cf7fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "WARNING: Training on CPU will be very slow. A GPU is recommended.\n"
     ]
    }
   ],
   "source": [
    "# --- Check for GPU ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device == torch.device(\"cpu\"):\n",
    "    print(\"WARNING: Training on CPU will be very slow. A GPU is recommended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba16733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e68e1281fc4a4888999def5995e1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a095d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model 'Salesforce/blip-image-captioning-base' and processor...\n",
      "Model and processor loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Model and Processor ---\n",
    "print(f\"Loading base model '{MODEL_ID}' and processor...\")\n",
    "try:\n",
    "    processor = BlipProcessor.from_pretrained(MODEL_ID)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "    model.to(device) # Move model to GPU if available\n",
    "    print(\"Model and processor loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model/processor: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f0d7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Custom Dataset Class ---\n",
    "class VizWizLocalDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, processor=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Path to the directory containing images (e.g., train/, val/).\n",
    "            annotation_file (str): Path to the JSON annotation file.\n",
    "            processor: The Hugging Face processor for potential pre-processing (optional here).\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_file = annotation_file\n",
    "        self.processor = processor # Store processor if needed later, though not used in __getitem__\n",
    "\n",
    "        print(f\"Loading annotations from: {self.annotation_file}\")\n",
    "        if not os.path.exists(annotation_file):\n",
    "             raise FileNotFoundError(f\"Annotation file not found: {annotation_file}\")\n",
    "        if not os.path.isdir(image_dir):\n",
    "             raise NotADirectoryError(f\"Image directory not found: {image_dir}\")\n",
    "\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            self.annotations_data = json.load(f)\n",
    "\n",
    "        self.samples = self._create_samples()\n",
    "        print(f\"Loaded {len(self.samples)} valid (image, caption) pairs.\")\n",
    "\n",
    "    def _create_samples(self):\n",
    "        samples = []\n",
    "        # Create a mapping from image_id to file_name\n",
    "        image_id_to_filename = {img['id']: img['file_name'] for img in self.annotations_data['images']}\n",
    "\n",
    "        # Iterate through annotations\n",
    "        for ann in self.annotations_data['annotations']:\n",
    "            # Skip rejected captions\n",
    "            if ann.get('is_rejected', False):\n",
    "                continue\n",
    "\n",
    "            image_id = ann['image_id']\n",
    "            caption = ann['caption']\n",
    "            filename = image_id_to_filename.get(image_id)\n",
    "\n",
    "            if filename and caption:\n",
    "                image_path = os.path.join(self.image_dir, filename)\n",
    "                if os.path.exists(image_path): # Check if image file actually exists\n",
    "                     samples.append({\"image_path\": image_path, \"text\": caption})\n",
    "                else:\n",
    "                     print(f\"Warning: Image file not found for annotation {ann['id']}: {image_path}\")\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image_path = sample[\"image_path\"]\n",
    "        caption = sample[\"text\"]\n",
    "\n",
    "        try:\n",
    "            # Load image using PIL\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}. Returning None.\")\n",
    "            # Need a way for the collator/trainer to handle this.\n",
    "            # Returning None might cause issues downstream.\n",
    "            # A better approach might be to return a placeholder or skip in collate_fn.\n",
    "            # For now, let's return the problematic path and text to be filtered later.\n",
    "            return {\"image\": None, \"text\": caption, \"error_path\": image_path}\n",
    "\n",
    "\n",
    "        # Return image object and text caption\n",
    "        return {\"image\": image, \"text\": caption}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ea6c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading local datasets...\n",
      "Loading annotations from: C:\\Users\\voutl\\OneDrive\\Documents\\LifeEase\\vizwiz dataset\\annotations\\train.json\n",
      "Loaded 113987 valid (image, caption) pairs.\n",
      "Loading annotations from: C:\\Users\\voutl\\OneDrive\\Documents\\LifeEase\\vizwiz dataset\\annotations\\val.json\n",
      "Loaded 37786 valid (image, caption) pairs.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Load Custom Datasets ---\n",
    "print(\"\\nLoading local datasets...\")\n",
    "try:\n",
    "    train_dataset = VizWizLocalDataset(image_dir=TRAIN_IMG_DIR, annotation_file=TRAIN_ANNOTATION_FILE)\n",
    "    eval_dataset = VizWizLocalDataset(image_dir=VAL_IMG_DIR, annotation_file=VAL_ANNOTATION_FILE)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"Please ensure the DATASET_ROOT_DIR and subdirectories (train/, val/, annotations/) are correct.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing custom datasets: {e}\")\n",
    "    traceback.print_exc()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718bd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing datasets using .map()...\n",
      "Converting Torch Datasets to Hugging Face Datasets for .map()...\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Preprocessing Function ---\n",
    "# Define how to process each example (image + caption) from the custom Dataset\n",
    "def preprocess_data(examples):\n",
    "    # Input `examples` is now a dictionary where keys ('image', 'text') map to lists\n",
    "    # coming from batches of __getitem__ results.\n",
    "    images = examples['image']\n",
    "    texts = examples['text']\n",
    "\n",
    "    # Filter out samples where image loading failed in __getitem__\n",
    "    valid_indices = [i for i, img in enumerate(images) if img is not None]\n",
    "    if len(valid_indices) != len(images):\n",
    "        print(f\"Warning: Filtering {len(images) - len(valid_indices)} samples due to image loading errors in this batch.\")\n",
    "    images = [images[i] for i in valid_indices]\n",
    "    texts = [texts[i] for i in valid_indices]\n",
    "\n",
    "    if not images: # If batch becomes empty after filtering\n",
    "        return {}\n",
    "\n",
    "    # Process images and tokenize text captions (as labels)\n",
    "    # Padding/truncation is handled by the processor/data collator\n",
    "    try:\n",
    "        inputs = processor(images=images, text=texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        # The processor prepares 'input_ids' and 'attention_mask' for the text (captions)\n",
    "        # These will serve as the labels for the language model head during training\n",
    "        inputs['labels'] = inputs['input_ids']\n",
    "        return inputs\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processor call: {e}\")\n",
    "        # Return empty dict if processing fails for the batch\n",
    "        return {}\n",
    "\n",
    "\n",
    "print(\"\\nPreprocessing datasets using .map()...\")\n",
    "# Apply preprocessing using .map() - requires datasets library even for custom torch Dataset\n",
    "# Alternatively, apply preprocessing within a custom data collator\n",
    "# Let's try keeping the .map() approach, which might need converting the torch Dataset\n",
    "# back to a datasets.Dataset temporarily or using a different approach.\n",
    "\n",
    "# **Alternative: Preprocessing within a custom Data Collator (often cleaner)**\n",
    "# We will skip .map() here and handle processing in the collator.\n",
    "# print(\"Skipping .map(), preprocessing will be handled by Data Collator.\")\n",
    "# train_dataset_processed = train_dataset\n",
    "# eval_dataset_processed = eval_dataset\n",
    "\n",
    "# **Keeping .map() approach (Requires datasets library installed)**\n",
    "# Need to convert torch Dataset to HF Dataset first to use .map() easily\n",
    "try:\n",
    "    from datasets import Dataset as HFDataset\n",
    "    # Convert custom torch Dataset to Hugging Face Dataset format\n",
    "    # This loads all data into memory, might be inefficient for very large datasets\n",
    "    # Consider iterating or generators if memory is an issue\n",
    "    print(\"Converting Torch Datasets to Hugging Face Datasets for .map()...\")\n",
    "    train_dict = {\"image\": [], \"text\": []}\n",
    "    for i in range(len(train_dataset)):\n",
    "        sample = train_dataset[i]\n",
    "        if sample[\"image\"] is not None: # Skip errors\n",
    "            train_dict[\"image\"].append(sample[\"image\"])\n",
    "            train_dict[\"text\"].append(sample[\"text\"])\n",
    "    hf_train_dataset = HFDataset.from_dict(train_dict)\n",
    "    del train_dict # Free memory\n",
    "\n",
    "    hf_eval_dataset = None\n",
    "    if eval_dataset:\n",
    "        eval_dict = {\"image\": [], \"text\": []}\n",
    "        for i in range(len(eval_dataset)):\n",
    "            sample = eval_dataset[i]\n",
    "            if sample[\"image\"] is not None: # Skip errors\n",
    "                eval_dict[\"image\"].append(sample[\"image\"])\n",
    "                eval_dict[\"text\"].append(sample[\"text\"])\n",
    "        hf_eval_dataset = HFDataset.from_dict(eval_dict)\n",
    "        del eval_dict # Free memory\n",
    "\n",
    "    print(\"Applying preprocessing function via .map()...\")\n",
    "    train_dataset_processed = hf_train_dataset.map(\n",
    "        preprocess_data,\n",
    "        batched=True,\n",
    "        remove_columns=[\"image\", \"text\"] # Remove original image/text columns after processing\n",
    "    )\n",
    "\n",
    "    eval_dataset_processed = None\n",
    "    if hf_eval_dataset:\n",
    "        eval_dataset_processed = hf_eval_dataset.map(\n",
    "            preprocess_data,\n",
    "            batched=True,\n",
    "            remove_columns=[\"image\", \"text\"]\n",
    "        )\n",
    "\n",
    "    # Set format back to PyTorch tensors\n",
    "    train_dataset_processed.set_format(\"torch\")\n",
    "    if eval_dataset_processed:\n",
    "        eval_dataset_processed.set_format(\"torch\")\n",
    "\n",
    "    print(\"Preprocessing complete.\")\n",
    "    # print(train_dataset_processed[0]) # Uncomment to check a processed example\n",
    "\n",
    "except ImportError:\n",
    "     print(\"\\nError: `datasets` library not installed. Cannot use .map() approach.\")\n",
    "     print(\"Please install `datasets` (`pip install datasets`) or implement preprocessing within a custom data collator.\")\n",
    "     exit()\n",
    "except Exception as e:\n",
    "     print(f\"\\nError during .map() preprocessing: {e}\")\n",
    "     traceback.print_exc()\n",
    "     exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f36e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Training Arguments ---\n",
    "# (Same as before)\n",
    "print(\"\\nSetting up training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2, # Keep only the last 2 checkpoints\n",
    "    evaluation_strategy=\"steps\" if eval_dataset_processed else \"no\", # Evaluate during training if eval set exists\n",
    "    eval_steps=SAVE_STEPS if eval_dataset_processed else None, # Evaluate every save_steps\n",
    "    load_best_model_at_end=True if eval_dataset_processed else False, # Load best model based on eval loss\n",
    "    remove_unused_columns=False, # Important: Keep False as map should have handled it\n",
    "    push_to_hub=False, # Set to True to push to Hugging Face Hub\n",
    "    report_to=\"tensorboard\", # Or \"wandb\", \"none\"\n",
    "    fp16=torch.cuda.is_available(), # Use mixed precision if GPU available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e686578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Data Collator ---\n",
    "# Default collator handles padding etc. for already processed batches\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcffd8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Initialize Trainer ---\n",
    "print(\"\\nInitializing Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_processed,\n",
    "    eval_dataset=eval_dataset_processed, # Pass None if no eval set\n",
    "    processor=processor, # Pass processor for saving\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cafe490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Start Training ---\n",
    "print(\"\\nStarting training...\")\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # --- 9. Save Final Model & Processor ---\n",
    "    print(\"\\nSaving fine-tuned model and processor...\")\n",
    "    trainer.save_model(OUTPUT_DIR) # Saves model weights & config\n",
    "    processor.save_pretrained(OUTPUT_DIR) # Saves processor config\n",
    "    # trainer.log_metrics(\"train\", train_result.metrics) # Log final metrics\n",
    "    # trainer.save_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_state() # Saves trainer state\n",
    "    print(f\"Fine-tuned model and processor saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "    # --- Optional: Evaluate Final Model ---\n",
    "    if eval_dataset_processed:\n",
    "        print(\"\\nEvaluating final model...\")\n",
    "        metrics = trainer.evaluate()\n",
    "        # trainer.log_metrics(\"eval\", metrics)\n",
    "        # trainer.save_metrics(\"eval\", metrics)\n",
    "        print(\"Evaluation metrics:\", metrics)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nFine-tuning script finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnxEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
